# Todo

- [ ] Coupling Term in Advection with each scheme. Currently coupling term is always centered (`K`). Should we add upwind/MUSCL coupling terms? Or is centered coupling sufficient for stability?
- [ ] Leverage to vector gradient, divergence, convection for Navier-Stokes solver.
- [ ] Make the matrix free kernels multi cpu friendly, multi gpu friendly (KernelAbstractions.jl, CUDA.jl, etc.) Your kernels are loop-based and local-stencil, so they are structurally good for threading. But current implementation is effectively single-threaded and uses shared mutable KernelWork, so it is not thread-safe as-is for parallel calls on one work. To be multi-core robust: add Threads.@threads over independent blocks/lines and use one work buffer per thread (or no shared scratch). Current code is written for CPU Vector + scalar indexing loops + sparse CSC assembled paths. It does not map directly to CUDA/AMDGPU kernels in its current form. To make it GPU-friendly: write dedicated device kernels (or KernelAbstractions), use device arrays, avoid host-side sparse ops in hot path, and redesign scratch handling for GPU memory. So: good computational pattern, but needs explicit threading/GPU backend work before being truly multi-CPU/GPU friendly.
- [ ] Make operators AD friendly. Kernel path (gradient!, laplacian!, convection! with KernelWork). Uses in-place mutation and KernelWork{T} with fixed element type from ops. If ops/work are Float64, Dual numbers cannot flow through scratch arrays. So this is generally not friendly to ForwardDiff/Zygote as-is. Assembled path (gradient_matrix, laplacian_matrix, convection_matrix). More AD-friendly conceptually (pure vector-returning style). But sparse ops + scheme branching can still be fragile depending on AD backend. Reverse-mode (Zygote). Mutation-heavy kernels are usually problematic without custom rrules. So currently not Zygote-friendly. If you want AD support, best next step is: add non-mutating pure wrappers (no KernelWork mutation visible), or provide custom ChainRules rrules/frules for the core ops.
- [ ] Coupling with ImplicitGlobalGrid.jl Assembled sparse path (G/H/Winv) is not a good match for ImplicitGlobalGrid (distributed global sparse assembly/solve is heavy and not the intended model). Matrix-free kernels are the right coupling target. What you need for IGG coupling: Domain decomposition with halos Split xω, xγ, and stacked flux arrays per rank. Exchange halo layers before each stencil application (dm!/dp!/sm!/dmT! along decomposed dims). Per-rank work buffers One KernelWork per rank/thread context (no shared mutable scratch). BC handling Interior rank boundaries use halo exchange, not physical BC. Apply BoxBC / AdvBoxBC only on global outer boundaries. Keep your duplicated-endpoint periodic convention consistent globally (n duplicated, seam at n-1 ↔ 1). Global reductions where needed Norms/CFL/diagnostics use MPI reductions. So: yes, CartesianOperators kernel mode can be coupled with ImplicitGlobalGrid, and that is the recommended direction.
